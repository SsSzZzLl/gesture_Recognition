# Author : SsSzZzLl
# -*- coding = utf-8 -*-
# @Time : 2025/10/31 ä¸Šåˆ11:55
# @Site : 
# @file : infer.py
# @Software : PyCharm


import os
import sys
import torch
import cv2
import numpy as np
from torchvision import transforms
from PIL import Image
import json

current_path = os.path.abspath(__file__)
backend_dir = os.path.dirname(current_path)
sys.path.append(backend_dir)

from models.base_cnn import BaseCNN
from models.attention_cnn import AttentionCNN
from models.resnet_mini import ResNetMini


def load_classes(classes_path):
    with open(classes_path, "r", encoding="utf-8") as f:
        classes = json.load(f)
    return {v: k for k, v in classes.items()}


def preprocess_image(image, target_size=(128, 128)):
    """é¢„å¤„ç†ï¼šä¸è®­ç»ƒ/éªŒè¯é›†ä¿æŒä¸€è‡´"""
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(target_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0)  # å¢åŠ batchç»´åº¦


def infer_camera(model, class_map, device):
    """æ‘„åƒå¤´å®æ—¶æ¨ç†"""
    cap = cv2.VideoCapture(0)  # 0=é»˜è®¤æ‘„åƒå¤´ï¼Œå¤šä¸ªæ‘„åƒå¤´å¯æ¢1ã€2ç­‰
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # è°ƒæ•´ç”»é¢å®½åº¦
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # è°ƒæ•´ç”»é¢é«˜åº¦

    print("ğŸ“¹ æ‘„åƒå¤´å®æ—¶éªŒè¯å·²å¯åŠ¨ï¼ŒæŒ‰ 'q' é”®é€€å‡º...")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
            break

        # ç”»é¢æ°´å¹³ç¿»è½¬ï¼ˆé•œåƒæ˜¾ç¤ºï¼Œæ›´ç¬¦åˆæ“ä½œä¹ æƒ¯ï¼‰
        frame = cv2.flip(frame, 1)
        # é¢„å¤„ç†ï¼šBGRâ†’RGB+Resize+å½’ä¸€åŒ–
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_tensor = preprocess_image(frame_rgb).to(device)

        # æ¨¡å‹é¢„æµ‹
        model.eval()
        with torch.no_grad():
            outputs = model(input_tensor)
            pred_label = torch.argmax(outputs, dim=1).item()
            pred_action = class_map[pred_label]
            confidence = torch.softmax(outputs, dim=1)[0][pred_label].item() * 100

        # åœ¨ç”»é¢ä¸Šç»˜åˆ¶ç»“æœï¼ˆç»¿è‰²æ–‡å­—=é«˜ç½®ä¿¡åº¦ï¼Œçº¢è‰²=ä½ç½®ä¿¡åº¦ï¼‰
        color = (0, 255, 0) if confidence > 70 else (0, 0, 255)
        cv2.putText(
            frame, f"Gesture: {pred_action}",
            (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3
        )
        cv2.putText(
            frame, f"Confidence: {confidence:.1f}%",
            (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2
        )

        # æ˜¾ç¤ºç”»é¢
        cv2.imshow("Gesture Recognition (Camera)", frame)

        # æŒ‰'q'é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    # é‡Šæ”¾èµ„æº
    cap.release()
    cv2.destroyAllWindows()
    print("ğŸ‘‹ å®æ—¶éªŒè¯å·²ç»“æŸ")


def infer_image(model, image_path, class_map, device):
    """å•å›¾ç‰‡æ¨ç†ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰"""
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡ï¼š{image_path}")
        return
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    input_tensor = preprocess_image(image_rgb).to(device)

    model.eval()
    with torch.no_grad():
        outputs = model(input_tensor)
        pred_label = torch.argmax(outputs, dim=1).item()
        pred_action = class_map[pred_label]
        confidence = torch.softmax(outputs, dim=1)[0][pred_label].item() * 100

    cv2.putText(
        image, f"Gesture: {pred_action} ({confidence:.1f}%)",
        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
    )
    cv2.imshow("Gesture Recognition (Image)", image)
    cv2.waitKey(0)
    cv2.imwrite("infer_image_result.jpg", image)
    print(f"ğŸ’¾ å›¾ç‰‡æ¨ç†ç»“æœå·²ä¿å­˜è‡³ï¼šinfer_image_result.jpg")
    cv2.destroyAllWindows()


def infer_video(model, video_path, class_map, device, save_path="infer_video_result.mp4"):
    """è§†é¢‘æ¨ç†ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•è¯»å–è§†é¢‘ï¼š{video_path}")
        return

    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))

    print(f"ğŸ¬ è§†é¢‘æ¨ç†ä¸­ï¼ŒæŒ‰ 'q' é”®æå‰é€€å‡º...")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_tensor = preprocess_image(frame_rgb).to(device)
        model.eval()
        with torch.no_grad():
            outputs = model(input_tensor)
            pred_label = torch.argmax(outputs, dim=1).item()
            pred_action = class_map[pred_label]
            confidence = torch.softmax(outputs, dim=1)[0][pred_label].item() * 100

        color = (0, 255, 0) if confidence > 70 else (0, 0, 255)
        cv2.putText(
            frame, f"Gesture: {pred_action} ({confidence:.1f}%)",
            (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2
        )

        cv2.imshow("Gesture Recognition (Video)", frame)
        writer.write(frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    writer.release()
    cv2.destroyAllWindows()
    print(f"ğŸ’¾ è§†é¢‘æ¨ç†ç»“æœå·²ä¿å­˜è‡³ï¼š{save_path}")


def main():
    # ---------------------- é…ç½®å‚æ•°ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼ï¼‰----------------------
    model_type = "resnet_mini"  # å¯é€‰ï¼šbase_cnn/attention_cnn/resnet_mini
    model_path = "E:/DoooooooooG/gesture_Recognition/backend/experiments/results/resnet_mini_optimized/è¿·ä½ ResNet_best.pth"
    classes_path = "E:/DoooooooooG/gesture_Recognition/data/processed/classes.json"
    input_type = "camera"  # å¯é€‰ï¼šcameraï¼ˆæ‘„åƒå¤´ï¼‰/ imageï¼ˆå›¾ç‰‡ï¼‰/ videoï¼ˆè§†é¢‘ï¼‰
    input_path = "test_image.jpg"  # input_typeä¸ºimage/videoæ—¶å¡«å†™è·¯å¾„
    # -----------------------------------------------------------------------------

    # è®¾å¤‡é…ç½®ï¼ˆè‡ªåŠ¨é€‰æ‹©GPU/CPUï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # åŠ è½½ç±»åˆ«æ˜ å°„ï¼ˆæ‰‹åŠ¿æ ‡ç­¾â†’åç§°ï¼‰
    try:
        class_map = load_classes(classes_path)
        num_classes = len(class_map)
    except Exception as e:
        print(f"âŒ åŠ è½½ç±»åˆ«æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return

    # åŠ è½½æ¨¡å‹
    try:
        if model_type == "base_cnn":
            model = BaseCNN(num_classes=num_classes).to(device)
        elif model_type == "attention_cnn":
            model = AttentionCNN(num_classes=num_classes).to(device)
        elif model_type == "resnet_mini":
            model = ResNetMini(num_classes=num_classes).to(device)
        else:
            raise ValueError("æ¨¡å‹ç±»å‹é”™è¯¯ï¼å¯é€‰ï¼šbase_cnn/attention_cnn/resnet_mini")

        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{model_type}")
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e}")
        return

    # é€‰æ‹©æ¨ç†æ¨¡å¼
    if input_type == "camera":
        infer_camera(model, class_map, device)
    elif input_type == "image":
        infer_image(model, input_path, class_map, device)
    elif input_type == "video":
        infer_video(model, input_path, class_map, device)
    else:
        print("âŒ è¾“å…¥ç±»å‹é”™è¯¯ï¼å¯é€‰ï¼šcamera/image/video")


if __name__ == "__main__":
    main()